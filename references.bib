@article{MAT,
      title={Molecule Attention Transformer}, 
      author={Lukasz Maziarka and Tomasz Danel and Sławomir Mucha and Krzysztof Rataj and Jacek Tabor and Stanisław Jastrzebski},
      year={2020},
      journal={arXiv},
      url={arXiv:2002.08264v1}
}

@article{operapKa,
author = {Mansouri, Kamel and Cariello, Neal F. and Korotcov, Alexandru and Tkachenko, Valery and Grulke, Chris M. and Sprankle, Catherine S. and Allen, David and Casey, Warren M. and Kleinstreuer, Nicole C. and Williams, Antony J.},
title = {Open-source QSAR models for pKa prediction using multiple machine learning approaches},
journal = {Journal of Cheminformatics},
volume = {11},
number = {60},
year = {2019},
doi = {10.1186/s13321-019-0384-1}
}

@misc{directepistemicunc,
  doi = {10.48550/ARXIV.2102.08501},
  url = {https://arxiv.org/abs/2102.08501},
  author = {Jain, Moksh and Lahlou, Salem and Nekoei, Hadi and Butoi, Victor and Bertin, Paul and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Yoshua},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DEUP: Direct Epistemic Uncertainty Prediction},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{alnigregress,
author = {Soleimany, Ava P. and Amini, Alexander and Goldman, Samuel and Rus, Daniela and Bhatia, Sangeeta N. and Coley, Connor W.},
title = {Evidential Deep Learning for Guided Molecular Property Prediction and Discovery},
journal = {ACS Central Science},
volume = {7},
number = {8},
pages = {1356-1367},
year = {2021},
doi = {10.1021/acscentsci.1c00546},
URL = { 
        https://doi.org/10.1021/acscentsci.1c00546
    
},
eprint = { 
        https://doi.org/10.1021/acscentsci.1c00546
    
}
}

@article{alreggreedysample,
title = {Active learning for regression using greedy sampling},
journal = {Information Sciences},
volume = {474},
pages = {90-105},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518307680},
author = {Dongrui Wu and Chin-Teng Lin and Jian Huang},
keywords = {Active learning, Regression, Greedy sampling, Driver drowsiness estimation},
abstract = {Regression problems are pervasive in real-world applications. Generally a substantial amount of labeled samples are needed to build a regression model with good generalization ability. However, many times it is relatively easy to collect a large number of unlabeled samples, but time-consuming or expensive to label them. Active learning for regression (ALR) is a methodology to reduce the number of labeled samples, by selecting the most beneficial ones to label, instead of random selection. This paper proposes two new ALR approaches based on greedy sampling (GS). The first approach (GSy) selects new samples to increase the diversity in the output space, and the second (iGS) selects new samples to increase the diversity in both input and output spaces. Extensive experiments on 10 UCI and CMU StatLib datasets from various domains, and on 15 subjects on EEG-based driver drowsiness estimation, verified their effectiveness and robustness.}
}

@article{alcompoundclass,
author = {Lang, Tobias and Flachsenberg, Florian and von Luxburg, Ulrike and Rarey, Matthias},
title = {Feasibility of Active Machine Learning for Multiclass Compound Classification},
journal = {Journal of Chemical Information and Modeling},
volume = {56},
number = {1},
pages = {12-20},
year = {2016},
doi = {10.1021/acs.jcim.5b00332},
    note ={PMID: 26740007},

URL = { 
        https://doi.org/10.1021/acs.jcim.5b00332
    
},
eprint = { 
        https://doi.org/10.1021/acs.jcim.5b00332
    
}

}
@article{alleadop,
author = {Konze, Kyle D. and Bos, Pieter H. and Dahlgren, Markus K. and Leswing, Karl and Tubert-Brohman, Ivan and Bortolato, Andrea and Robbason, Braxton and Abel, Robert and Bhat, Sathesh},
title = {Reaction-Based Enumeration, Active Learning, and Free Energy Calculations To Rapidly Explore Synthetically Tractable Chemical Space and Optimize Potency of Cyclin-Dependent Kinase 2 Inhibitors},
journal = {Journal of Chemical Information and Modeling},
volume = {59},
number = {9},
pages = {3782-3793},
year = {2019},
doi = {10.1021/acs.jcim.9b00367},
    note ={PMID: 31404495},

URL = { 
        https://doi.org/10.1021/acs.jcim.9b00367
    
},
eprint = { 
        https://doi.org/10.1021/acs.jcim.9b00367
    
}

}


@misc{alimage,
  doi = {10.48550/ARXIV.2007.11344},
  url = {https://arxiv.org/abs/2007.11344},
  author = {Hemmer, Patrick and Kühl, Niklas and Schöffer, Jakob},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DEAL: Deep Evidential Active Learning for Image Classification},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@misc{allanguage,
  doi = {10.48550/ARXIV.2104.08320},
  url = {https://arxiv.org/abs/2104.08320},
  author = {Margatina, Katerina and Barrault, Loïc and Aletras, Nikolaos},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {On the Importance of Effectively Adapting Pretrained Language Models for Active Learning},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@techreport{alreview1,
Author = {Burr Settles},
Institution = {University of Wisconsin--Madison},
Number = {1648},
Title = {Active Learning Literature Survey},
Type = {Computer Sciences Technical Report},
Year = {2009},
}

@article{alreview2,
author = {Sayin, Burcu and Krivosheev, Evgeny and Yang, Jie and Passerini, Andrea and Casati, Fabio},
title = {A review and experimental analysis of active learning over crowdsourced data},
journal = {Artificial Intelligence Review},
volume = {54},
pages = {5283--5305},
doi = {10.1007/s10462-021-10021-3},
year = {2021}
}

@article{lipinski1997experimental,
  title={Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings},
  author={Lipinski, Christopher A and Lombardo, Franco and Dominy, Beryl W and Feeney, Paul J},
  journal={Advanced drug delivery reviews},
  volume={23},
  number={1-3},
  pages={3--25},
  year={1997},
  publisher={Elsevier}
}

@article{bohacekChemSpace,
author = {Bohacek, Regine S. and McMartin, Colin and Guida, Wayne C.},
title = {The art and practice of structure-based drug design: A molecular modeling perspective},
journal = {Medicinal Research Reviews},
volume = {16},
number = {1},
pages = {3-50},
doi = {https://doi.org/10.1002/(SICI)1098-1128(199601)16:1<3::AID-MED1>3.0.CO;2-6},
year = {1996}
}

@article{htsnumbers,
author = {Szymanski, Pawel and Markowicz, Magdalena and Mikiciuk-Olasik, Elzbieta},
title = {Adaptation of High-Throughput Screening in Drug Discovery - Toxicological Screening Tests},
journal = {International Journal of Molecular Sciences},
volume = {31},
number = {1},
pages = {427--452},
doi = {10.3390/ijms13010427},
year = {2011}

}
   
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{rdkit,
	howpublished = {http://www.rdkit.org},
	key = {rdkit},
	note = {accessed November 6, 2017.},
	title = {{RDKit: Open-Source Cheminformatics.}}
}

@article{morgan1965generation,
  title={The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service.},
  author={Morgan, Harry L},
  journal={Journal of chemical documentation},
  volume={5},
  number={2},
  pages={107--113},
  year={1965},
  publisher={ACS Publications}
}

@misc{maccskeys,
    title={MACCS keys.},
    author={MDL Information Systems}

}

@article{smiles,
author = {Weininger, David},
title = {SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
journal = {Journal of Chemical Information and Computer Sciences},
volume = {28},
number = {1},
pages = {31-36},
year = {1988},
doi = {10.1021/ci00057a005},

URL = { 
        https://pubs.acs.org/doi/abs/10.1021/ci00057a005
    
},
eprint = { 
        https://pubs.acs.org/doi/pdf/10.1021/ci00057a005
    
}

}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@article{Chembl,
    author = {Mendez, David and Gaulton, Anna and Bento, A Patrícia and Chambers, Jon and De Veij, Marleen and Félix, Eloy and Magariños, María Paula and Mosquera, Juan F and Mutowo, Prudence and Nowotka, Michał and Gordillo-Marañón, María and Hunter, Fiona and Junco, Laura and Mugumbate, Grace and Rodriguez-Lopez, Milagros and Atkinson, Francis and Bosc, Nicolas and Radoux, Chris J and Segura-Cabrera, Aldo and Hersey, Anne and Leach, Andrew R},
    title = "{ChEMBL: towards direct deposition of bioassay data}",
    journal = {Nucleic Acids Research},
    volume = {47},
    number = {D1},
    pages = {D930-D940},
    year = {2018},
    month = {11},
    abstract = "{ChEMBL is a large, open-access bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012, 2014 and 2017 Nucleic Acids Research Database Issues. In the last two years, several important improvements have been made to the database and are described here. These include more robust capture and representation of assay details; a new data deposition system, allowing updating of data sets and deposition of supplementary data; and a completely redesigned web interface, with enhanced search and filtering capabilities.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gky1075},
    url = {https://doi.org/10.1093/nar/gky1075},
    eprint = {https://academic.oup.com/nar/article-pdf/47/D1/D930/27437436/gky1075.pdf},
}


@InProceedings{bigmodelgeneralize,
  title =    {Why do Larger Models Generalize Better? {A} Theoretical Perspective via the {XOR} Problem},
  author =       {Brutzkus, Alon and Globerson, Amir},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =    {822--830},
  year =     {2019},
  editor =   {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =   {97},
  series =   {Proceedings of Machine Learning Research},
  month =    {09--15 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v97/brutzkus19b/brutzkus19b.pdf},
  url =      {https://proceedings.mlr.press/v97/brutzkus19b.html},
  abstract =     {Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we provide theoretical and empirical evidence that, in certain cases, overparameterized convolutional networks generalize better than small networks because of an interplay between weight clustering and feature exploration at initialization. We demonstrate this theoretically for a 3-layer convolutional neural network with max-pooling, in a novel setting which extends the XOR problem. We show that this interplay implies that with overparamterization, gradient descent converges to global minima with better generalization performance compared to global minima of small networks. Empirically, we demonstrate these phenomena for a 3-layer convolutional neural network in the MNIST task.}
}

@article{pkahard,
    author = {Lee, Adam C. and Crippen, Gordon M.},
    title = {Predicting pKa},
    journal = {Journal of Chemical Information and Modeling},
    volume = {49},
    number = {9},
    pages = {2013-2033},
    year = {2009},
    doi = {10.1021/ci900209w},
        note ={PMID: 19702243},
    
    URL = { 
            https://doi.org/10.1021/ci900209w
        
    },
    eprint = { 
            https://doi.org/10.1021/ci900209w
        
    }
}
